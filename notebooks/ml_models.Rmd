---
title: 'SDM Benchmark Study Part 9: Fitting and Testing Common ML Models'
author: "Benton Tripp"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
    df_print: paged
  
---

<style>
.tocify-extend-page {
  height: 0 !important;
}
.post_navi {
  display: flex
}

.post_navi-label {
  font-size: 0.8em;
  opacity: 0.5
}

.post_navi .post_navi-item {
  padding: 0 2.2em;
  width: 50%;
  position: relative;
  color: inherit !important
}

.post_navi .nav_prev {
  text-align: left
}

.post_navi .nav_next {
  text-align: right
}

.post_navi .nav_prev .post_navi-arrow {
  left: 0
}

.post_navi .nav_next .post_navi-arrow {
  right: 0
}

.post_navi .post_navi-arrow {
  position: absolute;
  top: 50%;
  transform: translateY(-50%);
  font-size: 2.5em;
  opacity: 0.3
}

footer .wrapper {
  max-width: -webkit-calc(800px - (30px*2));
  max-width: calc(800px - (30px*2));
  margin-right: auto;
  margin-left: auto;
  padding-right: 30px;
  padding-left: 30px
}

@media screen and (max-width:800px) {
  footer .wrapper {
    max-width: -webkit-calc(800px - (30px));
    max-width: calc(800px - (30px));
    padding-right: 15px;
    padding-left: 15px
  }
}

footer .wrapper:after,
.footer-col-wrapper:after {
  content: "";
  display: table;
  clear: both
}

.svg-icon {
  width: 26px;
  height: 16px;
  display: inline-block;
  fill: #828282;
  padding-right: 5px;
  vertical-align: text-top
}

.social-media-list li+li {
  padding-top: 5px
}

.site-footer {
  border-top: 1px solid #e8e8e8;
  padding: 30px 0
}

.footer-heading {
  font-size: 18px;
  margin-bottom: 15px
}

.contact-list,
.social-media-list {
  list-style: none;
  margin-left: 0;
  width: 155px;
}

.footer-col-wrapper {
  font-size: 15px;
  color: #828282;
  margin-left: -15px
}

.footer-col {
  float: left;
  margin-bottom: 15px;
  padding-left: 15px
}

.footer-col-1 {
  width: 30%;
}

.footer-col-2 {
  width: 25%;
}

.footer-col-3 {
  width: 45%;
}

@media screen and (max-width:800px) {

  .footer-col-1,
  .footer-col-2 {
    width: -webkit-calc(50% - (30px/2));
    width: calc(50% - (30px/2))
  }

  .footer-col-3 {
    width: -webkit-calc(100% - (30px/2));
    width: calc(100% - (30px/2))
  }
}

@media screen and (max-width:600px) {
  .footer-col {
    float: none;
    width: -webkit-calc(100% - (30px/2));
    width: calc(100% - (30px/2))
  }
}
</style>

```{r setup, include=F, warning=F, message=F}
knitr::opts_chunk$set(echo = T, message=F, warning=F, cache=F, root.dir="..")
```

## Overview

Part 9 of the study presents a detailed analysis of common machine learning models in Species
Distribution Modeling (SDM), focusing on logistic regression, KNN, classification tree, random
forest, and XGBoost models. Following fitting the models, comparisons of these models against
the "traditional" IPP and MaxEnt models will be made. The evaluation involves examining their
performance through statistical tests, including ANOVA and Kruskal-Wallis, followed by
non-parametric pairwise comparisons and effect size assessments. This comprehensive analysis 
aims to highlight the strengths and limitations of each model, providing insights that could
influence model selection in future research endeavors in this field.

## Setup

```{r setup-2, results='hide'}

# Load libraries
library(sf)
library(terra)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(readr)
library(data.table)
library(knitr)
library(purrr)
library(caret)

# Set seed 
set.seed(19)

# Load pre-processed data
source("R/load_preprocessed_data.R")
# Load other utility function
source("R/get_objects.R")
# Load Variable Importance
source("R/load_variable_importance.R")
# Load model accuracy function
source("R/model_accuracy.R")

```

```{r setup-3}

# Get data into normal data frame format (not `spatstat`)
data <- states %>%
  set_names() %>%
  purrr::map(function(st) {
  # Get raster by state
  r <- rasters[[st]]
  species %>% 
    set_names() %>%
    purrr::map(function(spec) {
      # Load `spatstat` quad data
      Q <- readRDS(file.path("artifacts", "train_spatstat_Q_2",
                             paste0(st, "_", spec, "_Q.rds")))
      Q.test <- readRDS(file.path("artifacts", "test_spatstat_Q_2",
                                  paste0(st, "_", spec, "_Q.rds")))
      # Load presence/absence data
      pres.train <- data.table(
        x=Q$data$x, 
        y=Q$data$y,
        presence=factor(T, levels=c(F,T), 
                        labels=c("Absence", "Presence")))
      abs.train <- data.table(
        x=Q$dummy$x, 
        y=Q$dummy$y, 
        presence=factor(F, levels=c(F,T),
                        labels=c("Absence", "Presence")))
      pres.test <- data.table(
        x=Q.test$data$x, 
        y=Q.test$data$y,
        presence=factor(T, levels=c(F,T),
                        labels=c("Absence", "Presence")))
      abs.test <- data.table(
        x=Q.test$dummy$x, 
        y=Q.test$dummy$y,
        presence=factor(F, levels=c(F,T),
                        labels=c("Absence", "Presence")))
      purrr::walk(names(r), function(n) {
        pres.train[, (n) := terra::extract(r[[n]], 
                                           cbind(pres.train$x,
                                                 pres.train$y))]
        abs.train[, (n) := terra::extract(r[[n]], 
                                          cbind(abs.train$x,
                                                abs.train$y))]
        pres.test[, (n) := terra::extract(r[[n]], 
                                          cbind(pres.test$x,
                                                pres.test$y))]
        abs.test[, (n) := terra::extract(r[[n]], 
                                         cbind(abs.test$x,
                                               abs.test$y))]
      })
      list(
        train=data.table::rbindlist(l=list(pres.train, abs.train)) %>%
          na.omit(),
        test=data.table::rbindlist(l=list(pres.test, abs.test)) %>%
          na.omit()
      )
    })
  })

```

## Model Fitting

```{r ml-setup, results='hide'}

control <- trainControl(method="cv", 
                        number=5, 
                        classProbs = T)

train.test <- function(model.type, fname, spec.state, data, var.imp.data, 
                       control, tune.grid=NULL, cov.keep=50) {
  purrr::walk(1:nrow(spec.state), function(i) {
    spec <- spec.state[i, ]$species
    st <- spec.state[i, ]$state
    fit.path <- file.path(paste0("artifacts/models/", fname),
                          paste0(spec, "_", st, "_", fname, ".rds"))
    results.path <- file.path(paste0("artifacts/test_results/", fname),
                              paste0(spec, "_", st, "_", fname, ".rds"))
    if (!file.exists(results.path)) {
      d <- data[[st]][[spec]]$train
      features <- var.imp.data[[st]][[spec]]
      covariates.keep <- cov.keep
      spec.sens.check <- F
      while (!spec.sens.check) {
        # Create formula
        feats <- features$variable[1:covariates.keep] %>%
          purrr::keep(~!is.na(.x))
        covariates.keep <- length(feats)
        # Create formula
        .f <- feats %>%
          paste(., collapse=" + ") %>% 
          paste("presence ~", .) %>% 
          as.formula()
        # Generalized for different model types
        if (model.type == "logistic regression") {
          fit <- train(.f, data = d, 
                       method = "glm", 
                       family = "binomial",
                       trControl = control, 
                       metric = "Accuracy")
        } else if (model.type %in% c("tree", "knn", "random forest", "xgboost")) {
          .method <- case_when(model.type == "tree" ~ "rpart",
                               model.type %in% c("knn") ~ model.type,
                               model.type == "random forest" ~ "ranger",
                               model.type == "xgboost" ~ "xgbTree",
                               T ~ "tree")
          fit <- train(.f, data=d, 
                 method = .method,
                 trControl = control, 
                 tuneGrid = tune.grid,
                 metric = "Accuracy")
        } else {
          stop("Invalid model type")
        }
        # Cache model object
        get.object(
          obj=fit,
          file.name=paste0(spec, "_", st, "_", fname, ".rds"), 
          obj.path=paste0("artifacts/models/", fname)) %>%
          suppressWarnings()
        
        pred <- predict(fit, d, type="prob")$Presence
        train.results <- cbind(
          d, data.table(obs = ifelse(d$presence == "Presence", T, F),
                        p.obs = pred))
        optimal.threshold <- optimize.f1(train.results)
        cm <- get.acc(train.results, optimal.threshold)
        acc <- tibble(
          common.name=spec,
          state=st,
          covariate.count=covariates.keep,
          optimal.threshold=optimal.threshold 
        ) %>%
          cbind(as.list(c(cm$overall, cm$byClass)) %>% 
                  as_tibble()) %>%
          select(common.name:Accuracy, Sensitivity, Specificity, F1)
        cat("Train Results:\n Species:", spec, "\n",
            "State:", st, "\n",
            "Covariates:", covariates.keep, "\n",
            "Optimal Threshold:", optimal.threshold, "\n",
            "Accuracy:", acc$Accuracy, "\n",
            "F1:", acc$F1, "\n",
            "Sensitivity (TP Rate):", acc$Sensitivity, "\n",
            "Specificity (TN Rate):", acc$Specificity, "\n")
        if ((acc$Sensitivity == 1 & acc$Specificity == 0) | 
            (acc$Sensitivity == 0 & acc$Specificity == 1)) {
          cat("\tThe sensitivity/specificity is a 0/1 pair",
              "for covariates.keep ==", covariates.keep, "\n")
          spec.sens.check <- F
          file.remove(glm.path)
          covariates.keep <- covariates.keep - 1
          if (covariates.keep < 1) {
            stop("\tUnable to successfully fit a model given the data.\n")
          } 
          next
        } else {
          spec.sens.check <- T
        }
      }
      
      results <- get.object(
        {
          d.test <- data[[st]][[spec]]$test
          pred.test <- predict(fit, d.test, type="prob")
          test.results <- cbind(d.test, 
                                data.table(obs = ifelse(
                                  d.test$presence == "Presence", T, F),
                                  p.obs = pred.test$Presence))
          cm <- get.acc(test.results, optimal.threshold)
          test.acc <- tibble(
            common.name=spec,
            state=st,
            covariate.count=covariates.keep,
            optimal.threshold=optimal.threshold 
          ) %>%
            cbind(as.list(c(cm$overall, cm$byClass)) %>% 
                    as_tibble()) %>%
            select(common.name:Accuracy, Sensitivity, Specificity, F1)
          all.predictions <- rasters[[st]] %>% 
            as.data.table() %>%
            predict(fit, ., type="prob") %>%
            .$Presence
          list(
            test=test.results,
            train=train.results,
            all.preds=all.predictions,
            thresh=optimal.threshold,
            train.accuracy=acc,
            test.accuracy=test.acc
          )
        },
        file.name=paste0(spec, "_", st, "_", fname, ".rds"),
        obj.path=paste0("artifacts/test_results/", fname)
      )
      cat("\tFinished model for", spec, "in", st, "\n")
    }
  })
}

```

### Logistic Regression

```{r train-lr, results='hide'}

train.test(model.type="logistic regression", 
           fname="logreg", 
           spec.state, 
           data, 
           var.imp.data,
           control, 
           cov.keep=50)

```

### Classification Tree

```{r train-dt, results='hide'}

train.test(model.type="tree", 
           fname="tree", 
           spec.state, 
           data, 
           var.imp.data,
           control, 
           tune.grid=data.frame(cp=seq(0, 0.1, by = 0.01)),
           cov.keep=50)

```

### K-Nearest Neighbors

```{r train-knn, results='hide'}

train.test(model.type="knn", 
           fname="knn", 
           spec.state, 
           data, 
           var.imp.data,
           control, 
           tune.grid=data.frame(k=1:10),
           cov.keep=50)

```

### Random Forest

```{r train-rf, results='hide'}

train.test(model.type="random forest", 
           fname="randomforest", 
           spec.state, 
           data, 
           var.imp.data,
           control, 
           tune.grid=expand.grid(
             splitrule=c("extratrees", "gini"),
             mtry=c(1,5,10),
             min.node.size=c(1,5,10)
           ),
           cov.keep=50)

```

### XGBoost

```{r train-xgb, results='hide'}

train.test(model.type="xgboost", 
           fname="xgboost", 
           spec.state, 
           data, 
           var.imp.data,
           control, 
           tune.grid=expand.grid(
             nrounds = 300,
             eta = 0.1,
             gamma = c(0, 0.25, 0.5),
             max_depth = c(3, 6, 9),
             colsample_bytree = c(0.5, 1),
             min_child_weight = 1,
             subsample = c(0.5, 1)
           ),
           cov.keep=50) 

```

## Results

```{r get-results}

model.scores <- purrr::map_df(c("ipp_glm_mpl_2", "maxent", 
                          "logreg", "knn", "tree", 
                          "randomforest", "xgboost"), function(m) {
                            m.type <- case_when(
                              m == "ipp_glm_mpl_2" ~ "ipp",
                              m == "maxent" ~ "maxent",
                              m == "logreg" ~ "logistic regression",
                              m == "knn" ~ "knn",
                              m == "tree" ~ "classification tree",
                              m == "randomforest" ~ "random forest",
                              m == "xgboost" ~ "xgboost",
                              T ~ "")
  purrr::map_df(1:nrow(spec.state), function(i) {
    spec <- spec.state[i,]$species
    st <- spec.state[i,]$state
    results.path <- file.path(paste0("artifacts/test_results/", m),
                              paste0(spec, "_", st, "_", m, ".rds"))
    readRDS(results.path)$test.accuracy
  }) %>% mutate(model.type=m.type)
})


reshape.data <- function(data, metric) {
  # Ensure that the metric is one of the expected columns
  if (!metric %in% c("Accuracy", "Sensitivity", 
                     "Specificity", "F1")) {
    stop("Invalid metric specified.")
  }
  # Reshape the data
  data %>%
    select(common.name, state, model.type, !!sym(metric)) %>%
    mutate(model.type = tolower(gsub(" ", ".", model.type))) %>%
    tidyr::pivot_wider(names_from = model.type, 
                       values_from = !!sym(metric),
                names_sep=".",
                names_prefix=paste0(tolower(metric), "."))
}

metrics <- c("Accuracy", "Sensitivity", "Specificity", "F1") %>%
  set_names() %>%
  purrr::map(~reshape.data(model.scores, .x))

dt.metrics <- c("Accuracy", "Sensitivity", "Specificity", "F1") %>%
  set_names() %>%
  purrr::map(~{
    d <- metrics[[.x]]
    DT::datatable(
      d,
      filter='none',
      selection='none',
      rownames=F,
      options=list(
        scrollX=T,
        scrollY="600px",
        paging=F,
        searching=F,
        orderMulti=T,
        info=F,
        lengthChange = F
      )) %>%
      DT::formatStyle(columns=names(d), 
                      `font-size`="13px") %>%
      DT::formatSignif(3:ncol(d), digits=4) %>%
      htmltools::div(id=paste0(.x, "_datatable"), 
                     style=ifelse(.x=="Accuracy", "", 
                                  "visibility:hidden; height:0;"), .)
  })

htmltools::div(
  htmltools::tags$script(
    '$(document).ready(function(){
        $("#metric_selector").change(function(){
          var selectedMetric = $(this).val();
          // Hide datatbles
          $("[id$=_datatable]").css({"visibility": "hidden", "height": 0});
          // Show the selected datatable
          $("#" + selectedMetric + "_datatable").css({"visibility": "visible",
                                                      "height": "auto"});
        });
      });'
  ),
  htmltools::tags$select(id='metric_selector',
                         lapply(names(dt.metrics), function(met) {
                           htmltools::tags$option(value=met, met)
                         })), dt.metrics)

```

### Overall Performance by Model Type

```{r metrics-summary, fig.width=10, fig.height=8}

model.scores %>% setDT()
model.scores[, `:=` (
    model.type = factor(model.type,
                        levels=unique(model.scores$model.type)),
    common.name = factor(common.name,
                         levels=unique(model.scores$common.name)),
    state = factor(state, 
                   levels=unique(model.scores$state))
  )]

plt.data <- model.scores %>%
  melt(.,
       id.vars = c("common.name", "state", "model.type"), 
       measure.vars = c("Accuracy", "Sensitivity", 
                        "Specificity", "F1"),
       variable.name = "metric", 
       value.name = "metric.value")

ggplot(plt.data, 
         aes(x = metric, y = metric.value, fill = model.type)) +
    geom_boxplot(outlier.shape = "o") +
    stat_summary(aes(group = model.type), fun = mean, fill="darkred",
                 geom = "point", shape = 21, size = 2, color = "black",
                 position = position_dodge(width = 0.75)) +
    scale_fill_brewer(palette = "Dark2") + 
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_blank(),
          axis.title = element_blank(),
          panel.background = element_rect(fill = "gray"),
          plot.background = element_rect(fill = "white"),
          strip.placement = "inside") + 
    coord_flip() +
    facet_wrap(~metric, ncol=2, scales="free",
               strip.position="top")

```

### Performance by Model, State

```{r metrics-states, fig.width=10, fig.height=30}

ggplot(plt.data, 
         aes(x = metric, y = metric.value, fill = model.type)) +
    geom_boxplot(outlier.shape = "o") +
    stat_summary(aes(group = model.type), fun = mean, fill="darkred",
                 geom = "point", shape = 21, size = 2, color = "black",
                 position = position_dodge(width = 0.75)) +
    scale_fill_brewer(palette = "Dark2") + 
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_blank(),
          axis.title = element_blank(),
          panel.background = element_rect(fill = "gray"),
          plot.background = element_rect(fill = "white"),
          strip.placement = "inside") + 
    coord_flip() +
    facet_wrap(~metric + state, ncol=2, scales="free",
               strip.position="top")

```

### Performance by Model, Species

```{r metrics-species, fig.width=10, fig.height=40}

ggplot(plt.data, 
         aes(x = metric, y = metric.value, fill = model.type)) +
    geom_boxplot(outlier.shape = "o") +
    stat_summary(aes(group = model.type), fun = mean, fill="darkred",
                 geom = "point", shape = 21, size = 2, color = "black",
                 position = position_dodge(width = 0.75)) +
    scale_fill_brewer(palette = "Dark2") + 
    theme_minimal(base_size = 12) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_blank(),
          axis.title = element_blank(),
          panel.background = element_rect(fill = "gray"),
          plot.background = element_rect(fill = "white"),
          strip.placement = "inside") + 
    coord_flip() +
    facet_wrap(~metric + common.name, ncol=2, scales="free",
               strip.position="top")

```

### Discussion

Overall, the random forest model exhibits the highest average accuracy (94.64%) among all the
models, which indicates its robustness in predicting species distribution across various
datasets. This model also demonstrates high sensitivity (96.69%), suggesting its effectiveness
in correctly identifying true positives. In contrast, the ipp model shows the lowest overall
accuracy (89.53%), coupled with significantly lower sensitivity (69.92%), indicating a 
tendency to miss true positive cases. Notably, there are several instances of perfect accuracy
(100%). However, these appear to occur more frequently for bird species exhibiting a more
defined distribution (e.g., primarily coastal birds), in smaller sample areas.

A more granular analysis reveals variability in model performance across different bird 
species and states. One notable difference that appears to occur frequently is the more 
broadly distributed species, such as the Downy Woopecker, appear to have poorer 
performance when using thebaseline models, while the ML models performed significantly better.

In summary, while some models like random forest and KNN generally outperform others in 
terms of accuracy and sensitivity, the effectiveness of each model can vary considerably
depending on the bird species and the geographical context. Selecting the most
suitable model for a given SDM task requires careful consideration of the specific
characteristics of the dataset. 

## Model Performance Comparisons

The analysis of model accuracies initially uses Analysis of Variance (ANOVA), aimed at 
identifying significant differences across accuracy by model type. However, as shown later, 
the Shapiro-Wilk test indicates non-normality in ANOVA residuals, necessitating a shift to
non-parametric methods. These methods, including the Kruskal-Wallis test and subsequent Dunn’s
post-hoc tests, offer a more suitable approach for this data set. This part of the study is
crucial for evaluating the performance differences among models, clearly identifying those with
statistically significant variations in accuracy.

```{r anova-acc}

anova.res <- aov(Accuracy ~ model.type, data = model.scores)
summary.aov(anova.res) 

```

### Confirming ANOVA Assumptions

The Shapiro-Wilk test can be used to confirm whether the residuals of an ANOVA model are
normally distributed. The null hypothesis is that the data is normally distributed.
If $p < alpha = 0.05$, then the null hypothesis is rejected, and it can be concluded that the
residuals of the ANOVA are not normally distributed

```{r normality-anova-residuals}
shapiro.test(residuals(anova.res))
```

### Non-Parametric Alternative to ANOVA

The Kruskal-Wallis Rank Sum Test is a non-parametric alternative to one-way ANOVA. 
It's used to compare more than two groups and does not assume a normal distribution.
The null hypothesis for this test is that all groups have the same median, with a
threshold of 0.05.

```{r kruskal-wallis}
kruskal.test(Accuracy ~ model.type, data = model.scores)
```

The Kruskal-Wallis Chi-squared value (the test statistic for the Kruskal-Wallis test) is 
26.037. This is analogous to the F-statistic in ANOVA but based on ranks rather than means.
The Degrees of Freedom (df) is 6, which corresponds to the number of groups (model types)
minus one. It's an important parameter in determining the statistical significance.
The p-value is 0.0002191. This indicates the probability of obtaining test results at least 
as extreme as the ones observed, under the assumption that the null hypothesis is true.
Because the p-value is much less than the threshold of 0.05, the null hypothesis of the Kruskal-Wallis test is rejected. In other words, there is statistically significant evidence 
to suggest that at least one model type has a different median accuracy compared to the others.

### Non-Parametric Pairwise Comparisons

To identify which specific model types differ from each other, conduct post-hoc 
pairwise comparisons using the non-parametric Dunn's test.
    
```{r dunn}

# Dunn's test for pairwise comparisons
dunn.res <- dunn.test::dunn.test(model.scores$Accuracy, 
                                 model.scores$model.type, 
                                 method="bonferroni")

```
Each cell in the table represents the comparison between the model types in the
corresponding row and column. For instance, the first cell under "ipp" compares the
Classification Tree models with IPP. The values in each cell are the test statistic for 
Dunn's test. These values indicate the degree of difference between the pair of groups. 
The p-values are given below each test statistic. These indicate the probability of observing 
a test statistic as extreme as, or more extreme than, the one observed, assuming the null
hypothesis of no difference is true.

#### Significant Differences

Comparisons with a p-value less than 0.05/2 (adjusted for the Bonferroni correction) are
considered statistically significant. For example, the comparison between KNN and IPP shows
a p-value of 0.0025, which is significant. This suggests that there is a statistically
significant difference in the accuracy between these two model types.

#### No Significant Differences

Comparisons with a p-value greater than 0.05/2 are not considered statistically significant. 
For instance, the comparison between MaxEnt and IPP shows a p-value of 0.4646, which is not
significant.

### Effect Size 

To understand the magnitude of differences, not just the statistical significance, calculate
Cliff's Delta for each pair of model types. Cliff's Delta represents the difference between
the probability that a case randomly chosen from group 1 (i.e., model-type 1) has a higher
score than a randomly chosen subject from group 2, and the inverse. It's often called a 
"success rate difference".
    
```{r effectsize}

pairs <- expand.grid(
  treatment=unique(model.scores$model.type),
  control=unique(model.scores$model.type)
) %>% 
  filter(treatment != control)

eff.size <- purrr::map_df(1:nrow(pairs), ~{
  treatment <- pairs[.x,]$treatment
  control <- pairs[.x,]$control
  t.acc <- model.scores$Accuracy[model.scores$model.type == treatment]
  c.acc <- model.scores$Accuracy[model.scores$model.type == control]
  cd <- effsize::cliff.delta(t.acc, c.acc)
  data.table(
    treatment=treatment,
    control=control,
    delta=cd$estimate,
    lower=cd$conf.int[[1]],
    upper=cd$conf.int[[2]],
    variance=cd$var,
    magnitude=cd$magnitude
  )
})

eff.size

```
```{r effsize-matrix}
eff.m <- reshape2::acast(eff.size, treatment ~ control, value.var='delta', margins=F)
corrplot::corrplot(eff.m, diag = F)
```

Below are those pairwise comparisons deemed significant by Dunn's test, and their corresponding
effect size results (Cliff's Delta, Lower/Upper 95% C.I., etc.).

```{r effsize-pairwise-sig}

get.trtmnt <- function(comp) {
  stringr::str_split(comp, " - ")[[1]][[1]]
}

get.cntrl <- function(comp) {
  stringr::str_split(comp, " - ")[[1]][[2]]
}

sig.efsz <- data.table(
  comp=dunn.res$comparisons,
  p.adj=dunn.res$P.adjusted
) %>%
  .[, .(treatment=sapply(comp, get.trtmnt), 
        control=sapply(comp, get.cntrl),
        p.adj)] %>%
  eff.size[., on=.(treatment, control)] %>%
  .[p.adj <= 0.05]

sig.efsz
```

## Conclusion

The primary purpose, besides a general testing of methodologies for Species Distribution
Modeling, was to identify which -- if any -- "ML" models might perform better in SDM problems
than more commonly used models such as MaxEnt or IPP. Although it is impossible to confirm
whether one choice of model is always -- or even frequently -- better performing than another,
this experiment did show significant evidence that some models are more likely to perform
better than others. Specifically, KNN, Logistic Regression, Random Forest, and XGBoost models
all performed significantly better (in terms of accuracy) than that of the IPP models in
this experiment.

Again, this experiment was not exhaustive. Although an effort was made to include a variety
of bird species found in different environments and locations, 8 species of a single animal
in just 4 US states is most certainly not conclusive evidence of these results. However,
it does show that the tendency for researchers or scientists to just go with what has always
been done before (e.g., an IPP or MaxEnt model for SDM), might not always be the best option.


